{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train = pd.read_csv('../input/train.csv')\n",
    "    test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def one_hot(train_df, test_df):\n",
    "    \n",
    "    ntrain = len(train_df)\n",
    "    \n",
    "    # Remove useless variables\n",
    "    for c in train_df.columns.values[10::]:\n",
    "        if train_df[c].value_counts()[0] == ntrain:\n",
    "            train_df.drop(c, axis=1, inplace=True)\n",
    "            test_df.drop(c, axis=1, inplace=True)\n",
    "    \n",
    "    all_df = pd.concat([train_df, test_df])\n",
    "    all_df = all_df.set_index('ID')\n",
    "    \n",
    "    cat = [] \n",
    "    for c in all_df.columns:\n",
    "        if all_df[c].dtype == 'object':\n",
    "            cat.append(c)\n",
    "    print (cat)\n",
    "            \n",
    "    dummies = pd.get_dummies(all_df[cat])\n",
    "    all_df.drop(cat, axis=1, inplace=True)\n",
    "    all_df = pd.concat([all_df, dummies], axis=1)\n",
    "        \n",
    "    train_df = all_df.iloc[0:ntrain]\n",
    "    test_df = all_df.iloc[ntrain::].drop('y', axis=1)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8']\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = get_data()\n",
    "train_df, test_df = one_hot(train_df, test_df)\n",
    "\n",
    "y = train_df['y'].values\n",
    "train_df.drop('y', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        self.kbest = params.pop('kbest')\n",
    "        self.label = params.pop('label')\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "class XgbWrapper(object):\n",
    "    def __init__(self, seed=0, params=None):\n",
    "        self.param = params\n",
    "        self.nrounds = params.pop('nrounds')\n",
    "        self.kbest = params.pop('kbest')\n",
    "        self.label = params.pop('label')\n",
    "        self.gbdt = None\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        watchlist = [(dtrain, 'train')]\n",
    "        self.gbdt = xgb.train(self.param, dtrain, self.nrounds, watchlist, verbose_eval=False)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(xgb.DMatrix(x))\n",
    "    \n",
    "class KerasWrapper(object):\n",
    "    def __init__(self, model, kbest, label):\n",
    "        self.clf = model\n",
    "        self.kbest = kbest\n",
    "        self.label = label\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide Hyperparameters (I chose these as quick as possible after some cv experimentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params1 = {\n",
    "    'colsample_bytree': 0.3,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.8,\n",
    "    'learning_rate': 0.01,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma':.055,\n",
    "    'eval_metric': 'rmse',\n",
    "    'nrounds': 500,\n",
    "    'kbest':380,\n",
    "    'label':'GBM 1'\n",
    "}\n",
    "\n",
    "rf_params1 = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':100,\n",
    "    'max_features': 'sqrt',\n",
    "    'max_depth': 40,\n",
    "    'min_samples_split':10,\n",
    "    'min_samples_leaf':5,\n",
    "    'min_samples_leaf': 1,\n",
    "    'kbest':380,\n",
    "    'label':'Random Forest 1'\n",
    "}\n",
    "\n",
    "et_params1 = {\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators':100,\n",
    "        'max_features': 'sqrt',\n",
    "        'max_depth': 50,\n",
    "        'min_samples_split':10,\n",
    "        'min_samples_leaf':5,\n",
    "        'kbest':350,\n",
    "        'label':'Extra Randomized Trees 1'\n",
    "}\n",
    "\n",
    "lr_params1 = {\n",
    "    'label': 'Linear Regression 1',\n",
    "    'kbest':410\n",
    "}\n",
    "\n",
    "KBEST_KERAS = 400\n",
    "def build_keras():\n",
    "    \n",
    "    # number of nodes need at first layer of nn\n",
    "    input_dims = KBEST_KERAS\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dims, input_dim=input_dims, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    # Hidden layer\n",
    "    model.add(Dense(input_dims//2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    # Hidden layer\n",
    "    model.add(Dense(input_dims//2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    # Hidden layer\n",
    "    model.add(Dense(input_dims//2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    # Hidden layer adds linear output\n",
    "    model.add(Dense(input_dims//2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    # Output Layer.\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "              metrics=['mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFOLDS = 5\n",
    "\n",
    "xg1 = XgbWrapper(params=xgb_params1)\n",
    "keras_nn = KerasWrapper(KerasRegressor(build_fn=build_keras, epochs=75, \n",
    "        batch_size=20, verbose=False), kbest=KBEST_KERAS, label='Nueral Net 1')\n",
    "et1 = SklearnWrapper(clf=ExtraTreesRegressor, params=et_params1)\n",
    "rf1 = SklearnWrapper(clf=RandomForestRegressor, params=rf_params1)\n",
    "lr1 = SklearnWrapper(clf=LinearRegression, params=lr_params1)\n",
    "\n",
    "regs = [et1, xg1, keras_nn, rf1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess for NN or Linear Regression (unecessary b/c all our values are currently 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "train_df = sc.fit_transform(train_df)\n",
    "test_df = sc.fit_transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Extra Randomized Trees 1 scored 0.5488133609858593\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "GBM 1 scored 0.5671183098292236\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Nueral Net 1 scored 0.5285278860818452\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Random Forest 1 scored 0.5412753362743201\n"
     ]
    }
   ],
   "source": [
    "level_one_train = np.zeros((train_df.shape[0], len(regs)))\n",
    "level_one_test = np.zeros((test_df.shape[0], len(regs)))\n",
    "\n",
    "kf = KFold(NFOLDS, shuffle=True)\n",
    "for j, reg in enumerate(regs):\n",
    "        cv_scores = []\n",
    "        dataset_blend_test = np.zeros((test_df.shape[0], NFOLDS))\n",
    "        \n",
    "        # train and test are indexes\n",
    "        for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "            print(\"Fold {}\".format(i+1))\n",
    "            x_train, x_test = train_df[train_index], train_df[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            #ftest feature selection within fold\n",
    "            skb = SelectKBest(mutual_info_regression, k=reg.kbest)\n",
    "            x_train = skb.fit_transform(x_train, y_train)\n",
    "            x_test = skb.transform(x_test)\n",
    "            bestk_test_df = skb.transform(test_df)\n",
    "            \n",
    "            reg.fit(x_train, y_train)\n",
    "            y_submission = reg.predict(x_test)\n",
    "            #track scores of 'validation' sets\n",
    "            cv_scores.append(r2_score(y_test, y_submission))\n",
    "            # predict on all of the test data\n",
    "            test_preds = reg.predict(bestk_test_df)\n",
    "            # store predictions from full test data set\n",
    "            dataset_blend_test[:, i] = test_preds\n",
    "            # fill in train set with predictions from other 80% of data\n",
    "            level_one_train[test_index, j] = y_submission\n",
    "\n",
    "        print('{} scored {}'.format(reg.label, np.mean(cv_scores)))\n",
    "\n",
    "        level_one_test[:, j] = dataset_blend_test.mean(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When were trying to combine each others models near the end of the comp we would save the above level_one_train and level_one_test along with their local cross validation score and public score on the kaggle leaderboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would do several second level models, then just a few NN's and/or GBM's on the third level then just average their predictions on the fourth and final level to get our final results. However, that would take a while so I just used a GBM to create final predictions because I don't feel like tuning a bunch of models we won't want to use in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-r2:61.1731+1.30461\ttest-r2:61.6841+4.87426\n",
      "[25]\ttrain-r2:36.8449+0.787824\ttest-r2:37.1529+2.9167\n",
      "[50]\ttrain-r2:22.1139+0.474696\ttest-r2:22.3008+1.74412\n",
      "[75]\ttrain-r2:13.1943+0.283595\ttest-r2:13.3074+1.0388\n",
      "[100]\ttrain-r2:7.79093+0.168902\ttest-r2:7.85801+0.608648\n",
      "[125]\ttrain-r2:4.51435+0.0986645\ttest-r2:4.55475+0.351342\n",
      "[150]\ttrain-r2:2.5255+0.0563797\ttest-r2:2.55001+0.198071\n",
      "[175]\ttrain-r2:1.31682+0.0309934\ttest-r2:1.33204+0.108697\n",
      "[200]\ttrain-r2:0.582201+0.0166953\ttest-r2:0.591885+0.0601057\n",
      "[225]\ttrain-r2:0.135672+0.0102629\ttest-r2:0.142242+0.0409123\n",
      "[250]\ttrain-r2:-0.136728+0.0093062\ttest-r2:-0.131774+0.0389819\n",
      "[275]\ttrain-r2:-0.303112+0.0104391\ttest-r2:-0.299157+0.0420503\n",
      "[300]\ttrain-r2:-0.405012+0.0116387\ttest-r2:-0.401401+0.0452696\n",
      "[325]\ttrain-r2:-0.46758+0.0125893\ttest-r2:-0.463922+0.0473694\n",
      "[350]\ttrain-r2:-0.506259+0.0131183\ttest-r2:-0.502405+0.0486951\n",
      "[375]\ttrain-r2:-0.530367+0.0134596\ttest-r2:-0.526184+0.0494519\n",
      "[400]\ttrain-r2:-0.545504+0.0136814\ttest-r2:-0.541049+0.0498028\n",
      "[425]\ttrain-r2:-0.555157+0.0138176\ttest-r2:-0.550381+0.0500394\n",
      "[450]\ttrain-r2:-0.561367+0.0139175\ttest-r2:-0.556262+0.0502365\n",
      "[475]\ttrain-r2:-0.56547+0.013993\ttest-r2:-0.560056+0.0503223\n",
      "[500]\ttrain-r2:-0.568214+0.0140329\ttest-r2:-0.562464+0.0504077\n",
      "[525]\ttrain-r2:-0.570111+0.0140368\ttest-r2:-0.564056+0.0504253\n",
      "[550]\ttrain-r2:-0.571451+0.0140509\ttest-r2:-0.565138+0.0504095\n",
      "[575]\ttrain-r2:-0.572441+0.0140469\ttest-r2:-0.565832+0.0504051\n",
      "[600]\ttrain-r2:-0.573208+0.0140248\ttest-r2:-0.566332+0.0503994\n",
      "[625]\ttrain-r2:-0.57381+0.0140026\ttest-r2:-0.566666+0.0504165\n",
      "[650]\ttrain-r2:-0.574289+0.0139897\ttest-r2:-0.566874+0.0503968\n",
      "[675]\ttrain-r2:-0.574676+0.01397\ttest-r2:-0.567035+0.0503476\n",
      "[700]\ttrain-r2:-0.575016+0.0139587\ttest-r2:-0.567098+0.0502444\n",
      "[725]\ttrain-r2:-0.575318+0.0139536\ttest-r2:-0.56719+0.0501926\n",
      "[750]\ttrain-r2:-0.575573+0.0139443\ttest-r2:-0.567281+0.0501222\n",
      "[775]\ttrain-r2:-0.575809+0.0139369\ttest-r2:-0.567308+0.0501055\n",
      "[800]\ttrain-r2:-0.576022+0.0139331\ttest-r2:-0.567393+0.0500803\n"
     ]
    }
   ],
   "source": [
    "def xgb_r2_score(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'r2', -r2_score(labels, preds)\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.01,\n",
    "    'max_depth': 1,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'reg:linear',\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(level_one_train, y)\n",
    "dtest = xgb.DMatrix(level_one_test)\n",
    "\n",
    "best_rounds = xgb.cv(xgb_params, dtrain, num_boost_round=1000000, nfold=5, \n",
    "                     feval=xgb_r2_score, early_stopping_rounds=10, verbose_eval=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned the number of trees to grow above with cv\n",
    "# Now I need to increase # of trees by 20% b/c I used 80% training / 20% validation\n",
    "num_boost_rounds = int(best_rounds.shape[0]*1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds, verbose_eval=False)\n",
    "final_preds = final_model.predict(dtest)\n",
    "\n",
    "test_df = pd.read_csv('../input/test.csv')\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = test_df['ID']\n",
    "sub['y'] = final_preds\n",
    "sub = sub.set_index('ID')\n",
    "sub.to_csv('../output/first_stack.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaderboard score improves by about .003 or top 55% to 43% which is decent but could be improved significantly by improving base models accuracy and diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
